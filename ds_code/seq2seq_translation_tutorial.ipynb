{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Translation with a Sequence to Sequence Network and Attention\n",
    "*************************************************************\n",
    "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
    "\n",
    "In this project we will be teaching a neural network to translate from\n",
    "French to English.\n",
    "\n",
    "::\n",
    "\n",
    "    [KEY: > input, = target, < output]\n",
    "\n",
    "    > il est en train de peindre un tableau .\n",
    "    = he is painting a picture .\n",
    "    < he is painting a picture .\n",
    "\n",
    "    > pourquoi ne pas essayer ce vin delicieux ?\n",
    "    = why not try that delicious wine ?\n",
    "    < why not try that delicious wine ?\n",
    "\n",
    "    > elle n est pas poete mais romanciere .\n",
    "    = she is not a poet but a novelist .\n",
    "    < she not not a poet but a novelist .\n",
    "\n",
    "    > vous etes trop maigre .\n",
    "    = you re too skinny .\n",
    "    < you re all alone .\n",
    "\n",
    "... to varying degrees of success.\n",
    "\n",
    "This is made possible by the simple but powerful idea of the `sequence\n",
    "to sequence network <http://arxiv.org/abs/1409.3215>`__, in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "To improve upon this model we'll use an `attention\n",
    "mechanism <https://arxiv.org/abs/1409.0473>`__, which lets the decoder\n",
    "learn to focus over a specific range of the input sequence.\n",
    "\n",
    "**Recommended Reading:**\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and\n",
    "understand Tensors:\n",
    "\n",
    "-  http://pytorch.org/ For installation instructions\n",
    "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
    "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
    "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
    "\n",
    "\n",
    "It would also be useful to know about Sequence to Sequence networks and\n",
    "how they work:\n",
    "\n",
    "-  `Learning Phrase Representations using RNN Encoder-Decoder for\n",
    "   Statistical Machine Translation <http://arxiv.org/abs/1406.1078>`__\n",
    "-  `Sequence to Sequence Learning with Neural\n",
    "   Networks <http://arxiv.org/abs/1409.3215>`__\n",
    "-  `Neural Machine Translation by Jointly Learning to Align and\n",
    "   Translate <https://arxiv.org/abs/1409.0473>`__\n",
    "-  `A Neural Conversational Model <http://arxiv.org/abs/1506.05869>`__\n",
    "\n",
    "You will also find the previous tutorials on\n",
    ":doc:`/intermediate/char_rnn_classification_tutorial`\n",
    "and :doc:`/intermediate/char_rnn_generation_tutorial`\n",
    "helpful as those concepts are very similar to the Encoder and Decoder\n",
    "models, respectively.\n",
    "\n",
    "And for more, read the papers that introduced these topics:\n",
    "\n",
    "-  `Learning Phrase Representations using RNN Encoder-Decoder for\n",
    "   Statistical Machine Translation <http://arxiv.org/abs/1406.1078>`__\n",
    "-  `Sequence to Sequence Learning with Neural\n",
    "   Networks <http://arxiv.org/abs/1409.3215>`__\n",
    "-  `Neural Machine Translation by Jointly Learning to Align and\n",
    "   Translate <https://arxiv.org/abs/1409.0473>`__\n",
    "-  `A Neural Conversational Model <http://arxiv.org/abs/1506.05869>`__\n",
    "\n",
    "\n",
    "**Requirements**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "==================\n",
    "\n",
    "The data for this project is a set of many thousands of English to\n",
    "French translation pairs.\n",
    "\n",
    "`This question on Open Data Stack\n",
    "Exchange <http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
    "pointed me to the open translation site http://tatoeba.org/ which has\n",
    "downloads available at http://tatoeba.org/eng/downloads - and better\n",
    "yet, someone did the extra work of splitting language pairs into\n",
    "individual text files here: http://www.manythings.org/anki/\n",
    "\n",
    "The English to French pairs are too big to include in the repo, so\n",
    "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
    "separated list of translation pairs:\n",
    "\n",
    "::\n",
    "\n",
    "    I am cold.    J'ai froid.\n",
    "\n",
    ".. Note::\n",
    "   Download the data from\n",
    "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
    "   and extract it to the current directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN\n",
    "tutorials, we will be representing each word in a language as a one-hot\n",
    "vector, or giant vector of zeros except for a single one (at the index\n",
    "of the word). Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` to use to later replace rare words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. The files are all English → Other Language, so if we\n",
    "want to translate from Other Language → English I added the ``reverse``\n",
    "flag to reverse the pairs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "#     lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "#         read().strip().split('\\n')\n",
    "    lines = open('data/fra-eng/fra.txt', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we'll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation) and we're filtering to sentences that translate to\n",
    "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
    "earlier).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 160872 sentence pairs\n",
      "Trimmed to 12244 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4785\n",
      "eng 3116\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elle a de l assurance .', 'she s assertive .']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4785"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fra'"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.word2index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "^^^^^^^^^^^^^^^^^\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
    "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 49s (- 7m 29s) (1000 10%) 2.9800\n",
      "1m 41s (- 6m 45s) (2000 20%) 2.8544\n",
      "2m 34s (- 6m 0s) (3000 30%) 2.6945\n",
      "3m 25s (- 5m 7s) (4000 40%) 2.5739\n",
      "4m 12s (- 4m 12s) (5000 50%) 2.5496\n",
      "4m 58s (- 3m 19s) (6000 60%) 2.4590\n",
      "5m 45s (- 2m 27s) (7000 70%) 2.3941\n",
      "6m 34s (- 1m 38s) (8000 80%) 2.2910\n",
      "7m 23s (- 0m 49s) (9000 90%) 2.2498\n",
      "8m 9s (- 0m 0s) (10000 100%) 2.1630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126d52390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1259e5ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 10000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> il est en colere apres le monde .\n",
      "= he s angry at the world .\n",
      "< he is a good . . <EOS>\n",
      "\n",
      "> je suis trop fatigue pour penser .\n",
      "= i m too tired to think .\n",
      "< i m too old for your . <EOS>\n",
      "\n",
      "> j en ai marre de garder des secrets .\n",
      "= i m tired of keeping secrets .\n",
      "< i m afraid of his . . <EOS>\n",
      "\n",
      "> je suis nerveux et excite .\n",
      "= i m nervous and excited .\n",
      "< i m very with happy . <EOS>\n",
      "\n",
      "> je suis epuisee .\n",
      "= i am exhausted .\n",
      "< i m a . <EOS>\n",
      "\n",
      "> elle sourit a son bebe .\n",
      "= she smiled at her baby .\n",
      "< she saw him in his . <EOS>\n",
      "\n",
      "> je retourne au bureau .\n",
      "= i m going back to the office .\n",
      "< i m going to . . <EOS>\n",
      "\n",
      "> nous enquetons toujours sur la cause .\n",
      "= we re still investigating the cause .\n",
      "< we are all on . . <EOS>\n",
      "\n",
      "> nous sommes contents que vous soyez la .\n",
      "= we re glad you re here .\n",
      "< we re glad you re here . <EOS>\n",
      "\n",
      "> je ne suis plus marie .\n",
      "= i m not married anymore .\n",
      "< i m not allowed . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x123b71320>"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12d08f400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = elle a cinq ans de moins que moi .\n",
      "output = she is him him . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1259c6828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = elle est trop petit .\n",
      "output = she is too busy . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1259c67f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = je ne crains pas de mourir .\n",
      "output = i m not afraid . . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12384cf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = c est un jeune directeur plein de talent .\n",
      "output = he is a bit liar . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123892ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4785, 3116)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words, output_lang.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12244"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, n_iters, print_every=encoder1, attn_decoder1, 75000, 5000\n",
    "learning_rate=0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "random.seed(123)\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                  for i in range(n_iters)]\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 1]), torch.Size([5, 1])]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in training_pairs[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1\n",
    "training_pair = training_pairs[iter - 1]\n",
    "input_tensor = training_pair[0]\n",
    "target_tensor = training_pair[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter = 1\n",
    "training_pair = training_pairs[iter - 1]\n",
    "input_tensor = training_pair[0]\n",
    "target_tensor = training_pair[1]\n",
    "\n",
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion = \\\n",
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion\n",
    "max_length=MAX_LENGTH\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.]]])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length, target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([10, 256])\n",
      "1\n",
      "torch.Size([10, 256])\n",
      "2\n",
      "torch.Size([10, 256])\n",
      "3\n",
      "torch.Size([10, 256])\n",
      "4\n",
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion = \\\n",
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer, criterion\n",
    "max_length=MAX_LENGTH\n",
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    print(ei)\n",
    "    print(encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.9155,  0.6107,  3.9957,  1.6350,  4.4539,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000], grad_fn=<SumBackward1>),\n",
       " tensor(1, dtype=torch.uint8))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.sum(dim=-1), (encoder_outputs.sum(dim=-1)!=0).sum() == input_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion = \\\n",
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer, criterion\n",
    "max_length=MAX_LENGTH\n",
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "```\n",
    "\n",
    "created by\n",
    "```\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 4785, Embedding(4785, 256), GRU(256, 256))"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = encoder1\n",
    "self.hidden_size, input_lang.n_words, self.embedding, self.gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "iter = 1\n",
    "training_pair = training_pairs[iter - 1]\n",
    "input_tensor = training_pair[0]\n",
    "target_tensor = training_pair[1]\n",
    "\n",
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# for ei in range(input_length):\n",
    "ei = 0\n",
    "\n",
    "input, hidden = input_tensor[ei], encoder_hidden\n",
    "self.embedding(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = self.embedding(input).view(1, 1, -1) \n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embedded\n",
    "output, hidden = self.gru(output, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
    "\n",
    "Inputs: input, h_0\n",
    "input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n",
    "Outputs: output, h_n\n",
    "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\n",
    "\n",
    "Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    "\n",
    "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above,\n",
    "\n",
    "Inputs: input, h_0\n",
    "input of shape (seq_len, batch, input_size)\n",
    "seq_len, batch, input_size = 1, 1, 256\n",
    "\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "batch, hidden_size = 1, 256\n",
    "\n",
    "Outputs: output, h_n\n",
    "output of shape (seq_len, batch, num_directions * hidden_size)\n",
    "seq_len, batch, num_directions * hidden_size = 1, 1, 1*256\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "num_layers * num_directions, batch, hidden_size = 1*1, 1, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "loss = 0\n",
    "\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden = encoder_hidden\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "di 0\n",
      "torch.Size([1, 1]) torch.Size([1, 1, 256]) torch.Size([10, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256]) torch.Size([1, 10])\n",
      "di 1\n",
      "torch.Size([1]) torch.Size([1, 1, 256]) torch.Size([10, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256]) torch.Size([1, 10])\n",
      "di 2\n",
      "torch.Size([1]) torch.Size([1, 1, 256]) torch.Size([10, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256]) torch.Size([1, 10])\n",
      "di 3\n",
      "torch.Size([1]) torch.Size([1, 1, 256]) torch.Size([10, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256]) torch.Size([1, 10])\n",
      "di 4\n",
      "torch.Size([1]) torch.Size([1, 1, 256]) torch.Size([10, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "# Teacher forcing: Feed the target as the next input\n",
    "for di in range(target_length):\n",
    "    print('di', di)\n",
    "    print(decoder_input.shape, decoder_hidden.shape, encoder_outputs.shape)\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_hidden, encoder_outputs)\n",
    "    print(decoder_output.shape, decoder_hidden.shape, decoder_attention.shape)\n",
    "    loss += criterion(decoder_output, target_tensor[di])\n",
    "    decoder_input = target_tensor[di]  # Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3116]), torch.Size([1, 1, 256]), torch.Size([1, 10]))"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_hidden = encoder_hidden\n",
    "di = 0\n",
    "decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "decoder_output.shape, decoder_hidden.shape, decoder_attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_hidden, encoder_outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 3116, 0.1, 10)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self=decoder\n",
    "self.hidden_size,self.output_size,self.dropout_p,self.max_length,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(3116, 256),\n",
       " Linear(in_features=512, out_features=10, bias=True),\n",
       " Linear(in_features=512, out_features=256, bias=True),\n",
       " Dropout(p=0.1),\n",
       " GRU(256, 256),\n",
       " Linear(in_features=256, out_features=3116, bias=True))"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.embedding,self.attn,self.attn_combine,self.dropout,self.gru,self.out,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), torch.Size([1, 1, 256]), torch.Size([10, 256]))"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "loss = 0\n",
    "\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "# in decoder.forward() \n",
    "input, hidden, encoder_outputs = decoder_input, decoder_hidden, encoder_outputs\n",
    "input.shape, hidden.shape, encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`embedded = self.embedding(input).view(1, 1, -1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.embedding(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = self.embedding(input).view(1, 1, -1)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = self.dropout(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attn_weights = F.softmax(\n",
    "    self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 256]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded[0].shape, hidden[0].shape, torch.cat((embedded[0], hidden[0]), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=10, bias=True)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.attn(torch.cat((embedded[0], hidden[0]), 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = F.softmax(\n",
    "    self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                         encoder_outputs.unsqueeze(0))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 10]), torch.Size([1, 10, 256]))"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.unsqueeze(0).shape, encoder_outputs.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                         encoder_outputs.unsqueeze(0))\n",
    "attn_applied.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output = torch.cat((embedded[0], attn_applied[0]), 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 256]))"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded[0].shape, attn_applied[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output = self.attn_combine(output).unsqueeze(0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=256, bias=True)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.attn_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.attn_combine(output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = self.attn_combine(output).unsqueeze(0)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = F.relu(output)\n",
    "output, hidden = self.gru(output, hidden)\n",
    "output.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output = F.log_softmax(self.out(output[0]), dim=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=3116, bias=True)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3116])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.out(output[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3116])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human → Machine (e.g. IOT commands)\n",
    "   -  Chat → Response\n",
    "   -  Question → Answer\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No attention - Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "# trainIters(encoder1, decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 3s (- 0m 34s) (100 10%) 4.1543\n",
      "0m 8s (- 0m 32s) (200 20%) 3.5679\n",
      "0m 12s (- 0m 28s) (300 30%) 3.5292\n",
      "0m 16s (- 0m 25s) (400 40%) 3.6525\n",
      "0m 21s (- 0m 21s) (500 50%) 3.5490\n",
      "0m 25s (- 0m 17s) (600 60%) 3.4550\n",
      "0m 30s (- 0m 12s) (700 70%) 3.3209\n",
      "0m 34s (- 0m 8s) (800 80%) 3.2924\n",
      "0m 39s (- 0m 4s) (900 90%) 3.3942\n",
      "0m 43s (- 0m 0s) (1000 100%) 3.3473\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1, 1000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "#         decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "#             decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "#         return decoded_words, decoder_attentions[:di + 1]        \n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> vous etes surmene .\n",
      "= you re overworked .\n",
      "< you re a . . <EOS>\n",
      "\n",
      "> tu es plus grande que moi .\n",
      "= you are taller than me .\n",
      "< you re a . . <EOS>\n",
      "\n",
      "> je suis desolee mon pere est elimine .\n",
      "= i m sorry my father is out .\n",
      "< you re a . . <EOS>\n",
      "\n",
      "> je suis une sorte de solitaire .\n",
      "= i m kind of a loner .\n",
      "< i m not a . . <EOS>\n",
      "\n",
      "> elle m a envoye un telegramme urgent .\n",
      "= she sent me an urgent telegram .\n",
      "< i m not a . . <EOS>\n",
      "\n",
      "> emotionnellement je suis vide .\n",
      "= i m emotionally drained .\n",
      "< i m not a . . <EOS>\n",
      "\n",
      "> nous y sommes prets .\n",
      "= we re ready for this .\n",
      "< you re a . . <EOS>\n",
      "\n",
      "> je commence a apercevoir un motif .\n",
      "= i m beginning to see a pattern .\n",
      "< i m not a . . <EOS>\n",
      "\n",
      "> j ai une assuetude .\n",
      "= i m addicted .\n",
      "< i m not a . . <EOS>\n",
      "\n",
      "> je me sens assez malheureux a ce sujet .\n",
      "= i m quite unhappy about it .\n",
      "< i m not a . . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No attention - Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "trainIters(encoder1, decoder1, 75000, print_every=5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4785, 3116)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words, output_lang.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12244"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, n_iters, print_every=encoder1, decoder1, 75000, 5000\n",
    "learning_rate=0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "random.seed(123)\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                  for i in range(n_iters)]\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 1]), torch.Size([5, 1])]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in training_pairs[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1\n",
    "training_pair = training_pairs[iter - 1]\n",
    "input_tensor = training_pair[0]\n",
    "target_tensor = training_pair[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter = 1\n",
    "training_pair = training_pairs[iter - 1]\n",
    "input_tensor = training_pair[0]\n",
    "target_tensor = training_pair[1]\n",
    "\n",
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion = \\\n",
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer, criterion\n",
    "max_length=MAX_LENGTH\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0.]]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length, target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n",
      "1\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n",
      "2\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n",
      "3\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n",
      "4\n",
      "torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion = \\\n",
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion\n",
    "max_length=MAX_LENGTH\n",
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    print(ei)\n",
    "    print(encoder_output.shape, encoder_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0], dtype=torch.uint8),\n",
       " tensor(1, dtype=torch.uint8))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.sum(dim=-1)!=0, (encoder_outputs.sum(dim=-1)!=0).sum() == input_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion = \\\n",
    "input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer, criterion\n",
    "max_length=MAX_LENGTH\n",
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "```\n",
    "\n",
    "created by\n",
    "```\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 4785, Embedding(4785, 256), GRU(256, 256))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = encoder1\n",
    "self.hidden_size, input_lang.n_words, self.embedding, self.gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "iter = 1\n",
    "training_pair = training_pairs[iter - 1]\n",
    "input_tensor = training_pair[0]\n",
    "target_tensor = training_pair[1]\n",
    "\n",
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# for ei in range(input_length):\n",
    "ei = 0\n",
    "\n",
    "input, hidden = input_tensor[ei], encoder_hidden\n",
    "self.embedding(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = self.embedding(input).view(1, 1, -1) \n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embedded\n",
    "output, hidden = self.gru(output, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
    "\n",
    "Inputs: input, h_0\n",
    "input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details.\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n",
    "Outputs: output, h_n\n",
    "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.\n",
    "\n",
    "Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    "\n",
    "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above,\n",
    "\n",
    "Inputs: input, h_0\n",
    "input of shape (seq_len, batch, input_size)\n",
    "seq_len, batch, input_size = 1, 1, 256\n",
    "\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "num_layers = 1\n",
    "num_directions = 1\n",
    "batch, hidden_size = 1, 256\n",
    "\n",
    "Outputs: output, h_n\n",
    "output of shape (seq_len, batch, num_directions * hidden_size)\n",
    "seq_len, batch, num_directions * hidden_size = 1, 1, 1*256\n",
    "\n",
    "h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "num_layers * num_directions, batch, hidden_size = 1*1, 1, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "loss = 0\n",
    "\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden = encoder_hidden\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "di 0\n",
      "torch.Size([1, 1]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1])\n",
      "di 1\n",
      "torch.Size([1]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1])\n",
      "di 2\n",
      "torch.Size([1]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1])\n",
      "di 3\n",
      "torch.Size([1]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1])\n",
      "di 4\n",
      "torch.Size([1]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1, 1, 256])\n",
      "torch.Size([1, 3116]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "# Teacher forcing: Feed the target as the next input\n",
    "for di in range(target_length):\n",
    "    print('di', di)\n",
    "    print(decoder_input.shape, decoder_hidden.shape)\n",
    "    decoder_output, decoder_hidden = decoder(\n",
    "        decoder_input, decoder_hidden)\n",
    "    print(decoder_output.shape, decoder_hidden.shape)\n",
    "    loss += criterion(decoder_output, target_tensor[di])\n",
    "    print(decoder_output.shape, target_tensor[di].shape)\n",
    "    decoder_input = target_tensor[di]  # Teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_hidden = encoder_hidden\n",
    "di = 0\n",
    "decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3116]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape, decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,\n",
       " Embedding(3116, 256),\n",
       " GRU(256, 256),\n",
       " Linear(in_features=256, out_features=3116, bias=True),\n",
       " LogSoftmax())"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self=decoder\n",
    "self.hidden_size, self.embedding, self.gru, self.out, self.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "loss = 0\n",
    "\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_hidden = encoder_hidden\n",
    "input, hidden = decoder_input, decoder_hidden\n",
    "output = self.embedding(input).view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.embedding(input).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = F.relu(output)\n",
    "output, hidden = self.gru(output, hidden)\n",
    "output.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3116])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.out(output[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3116])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = self.softmax(self.out(output[0]))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch04]",
   "language": "python",
   "name": "conda-env-pytorch04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
