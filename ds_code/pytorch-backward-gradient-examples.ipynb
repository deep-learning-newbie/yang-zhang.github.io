{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to post: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the \"gradient\" argument in Pytorch's \"backward\" function work - explained by examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is some examples for the `gradient` argument in Pytorch's `backward` function. The math of `backward(gradient)` is explained in this [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) and these threads ([thread-1](https://discuss.pytorch.org/t/gradient-argument-in-out-backward-gradient/12742), [thread-2](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)), along with some examples. Those were very helpful, but I wish there were more examples on how the numbers in the example correspond to the math, to help me more easily understand. I could not find many such examples so I will make some and write them here, so that I can look back when I forget this in two weeks.\n",
    "\n",
    "In the examples, I run code in torch, write down the math, and run the math in numpy, and show that the torch result matches the math/numpy result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how Pytorch [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) explains the math:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, if you have a vector valued function y⃗ =f(x⃗ ), then the gradient of y⃗  with respect to x⃗  is a Jacobian matrix:\n",
    "\n",
    "J=⎛⎝⎜⎜⎜⎜∂y1∂x1⋮∂ym∂x1⋯⋱⋯∂y1∂xn⋮∂ym∂xn⎞⎠⎟⎟⎟⎟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will make examples of `x` and `y=f(x)` (we omit the arrow-hats of `x` and `y` above), and manually calculate Jacobian `J`.* \n",
    "\n",
    "Pytorch tutorial goes on with the explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, torch.autograd is an engine for computing vector-Jacobian product. That is, given any vector v=(v1v2⋯vm)T, compute the product vT⋅J. If v happens to be the gradient of a scalar function l=g(y⃗ ), that is, v=(∂l∂y1⋯∂l∂ym)T, then by the chain rule, the vector-Jacobian product would be the gradient of l with respect to x⃗ :\n",
    "\n",
    "JT⋅v=⎛⎝⎜⎜⎜⎜∂y1∂x1⋮∂y1∂xn⋯⋱⋯∂ym∂x1⋮∂ym∂xn⎞⎠⎟⎟⎟⎟⎛⎝⎜⎜⎜⎜∂l∂y1⋮∂l∂ym⎞⎠⎟⎟⎟⎟=⎛⎝⎜⎜⎜⎜∂l∂x1⋮∂l∂xn⎞⎠⎟⎟⎟⎟\n",
    "(Note that vT⋅J gives a row vector which can be treated as a column vector by taking JT⋅v.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above basically says: if you pass `vᵀ` as the `gradient` argument in `y.backward(gradient)`, then `y.backward(gradient=vᵀ)` will give you not `J` but `vᵀ・J` as `x.grad`.\n",
    "\n",
    "*We will make examples of `vᵀ`, calculate `vᵀ・J` in numpy, and confirm that the result is the same as `x.grad` after calling `y.backward(gradient=vᵀ)`.*\n",
    "\n",
    "All good? Let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is scalar, output is scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the simplest example where `x = 1` and `y = x**2` are both scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(1., requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "y.backward() # this is the same as y.backward(tensor(1.))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manully calculate `J`. In this case, calculus says `J` has the value `2*x`, and its shape is 1x1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2.*x]])\n",
    "print('J:', J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we did not pass the `gradient` argument to `backward()`, and this defaults to passing the value `1`. As a reminder, `vᵀ` is our `gradient`, and here it has shape 1x1 with value `1` in it. We can confirm that `vᵀ・J` gives the same result as `x.grad`. All good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[1.]]\n",
      "vᵀ・J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([[1.,]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is scalar, output is scalar, non-default gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep everything else the same but pass a non-default `gradient` with the value `100` to `backward()` that does not have the value `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(200.)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(1., requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "gradient_value = 100.\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as setting the value `100` for `vᵀ`, and we can see `vᵀ・J` still matches `x.grad`. Still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n",
      "vᵀ: [[100.]]\n",
      "vᵀ・J: [[200.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2.*x]])\n",
    "print('J:', J)\n",
    "\n",
    "vᵀ = array([[gradient_value,]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is vector, output is scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at a slightly more interesting example where `x = [1, 2]` is a vector and `y = sum(x)` is a scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor(3., grad_fn=<AddBackward0>)\n",
      "x.grad: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "print('y:', y)\n",
    "y.backward() \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manully calculate `J`. In this case, calculus says `J` has the value $\\sadkfaisoej$, and its shape is 2x1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2.*x]])\n",
    "print('J:', J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastaiv1-cpu",
   "language": "python",
   "name": "fastaiv1-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
