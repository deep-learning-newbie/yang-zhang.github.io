{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to post: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the \"gradient\" argument in Pytorch's \"backward\" function work - explained by examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is some examples for the `gradient` argument in Pytorch's `backward` function. The math of `backward(gradient)` is explained in this [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) and these threads ([thread-1](https://discuss.pytorch.org/t/gradient-argument-in-out-backward-gradient/12742), [thread-2](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)), along with some examples. Those were very helpful, but I wish there were more examples on how the numbers in the example correspond to the math, to help me more easily understand. I could not find many such examples so I will make some and write them here, so that I can look back when I forget this in two weeks.\n",
    "\n",
    "In the examples, I run code in torch, write down the math, and run the math in numpy, and show that the torch result matches the math/numpy result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how Pytorch [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients) explains the math:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert picture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will make examples of $x$ and $y=f(x)$ (we omit the arrow-hats of $x$ and $y$ above), and manually calculate Jacobian $J$.* \n",
    "\n",
    "Pytorch tutorial goes on with the explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert picture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above basically says: if you pass $v^T$ as the `gradient` argument, then `y.backward(gradient)` will give you not $J$ but $v^T \\cdot J$ as the result of `x.grad`.\n",
    "\n",
    "*We will make examples of $v^T$, calculate $v^T \\cdot J$ in numpy, and confirm that the result is the same as `x.grad` after calling `y.backward(gradient)` where `gradient` is $v^T$.*\n",
    "\n",
    "All good? Let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is scalar, output is scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a simple example where $x=1$ and $y = x^2$ are both scalar. \n",
    "\n",
    "In pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(1., requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "y.backward() # this is the same as y.backward(tensor(1.))\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now manually calculate Jacobian $J$. In this case $x$ and $y$ are both scalar (each only has one component $x_1$ and $y_1$ respectively). And we have\n",
    "\n",
    "$$\n",
    "J = [\\partial y_1 / \\partial x_1] = [\\partial y / \\partial x] = [2x]\n",
    "$$\n",
    "\n",
    "In numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we did not pass the `gradient` argument to `backward()`, and this defaults to passing the value 1. As a reminder, $v^T$ is our `gradient` with value 1. We can confirm that $v^T \\cdot J$ gives the same result as `x.grad`. All good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[1]]\n",
      "vᵀ・J: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([[1,]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is scalar, output is scalar, non-default gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep everything else the same but pass a non-default `gradient` with the value 100 to `backward()` instead of the default value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(1., requires_grad=True)\n",
      "y: tensor(1., grad_fn=<PowBackward0>)\n",
      "x.grad: tensor(200.)\n"
     ]
    }
   ],
   "source": [
    "x = tensor(1., requires_grad=True)\n",
    "print('x:', x)\n",
    "y = x**2\n",
    "print('y:', y)\n",
    "gradient_value = 100.\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as setting the value `100` for $v^T$, and we can see $v^T \\cdot J$ still matches `x.grad`. Still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: [[2.]]\n",
      "vᵀ: [[100.]]\n",
      "vᵀ・J: [[200.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.detach().numpy()\n",
    "J = array([[2*x]])\n",
    "print('J:', J)\n",
    "\n",
    "vᵀ = array([[gradient_value,]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is vector, output is scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at a more interesting example where $x=[x_1,x_2]=[1,2]$ is a vector and $y=\\sum x_i$ is a scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor(3., grad_fn=<AddBackward0>)\n",
      "x.grad: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "print('y:', y)\n",
    "y.backward() \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now manually calculate Jacobian $J$. In this case since $x$ is a vector with components $x_1$ and $x_2$, and $y=x_1+x_2$ is a scalar, so we have\n",
    "\n",
    "$$\n",
    "J =  \n",
    "\\left( \\begin{array}{c}\n",
    "\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_1}{\\partial x_2}\n",
    "\\end{array} \\right)\n",
    "= \n",
    "\\left( \\begin{array}{c} 1,1\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "In numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      "[[1 1]]\n"
     ]
    }
   ],
   "source": [
    "J = array([[1, 1]])\n",
    "print('J:')\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we did not pass the `gradient` argument to `backward()`, and this defaults to passing the value 1, i.e., $v^T$ has value 1. We can confirm that $v^T \\cdot J$ gives the same result as `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vᵀ: [[1]]\n",
      "vᵀ・J: [[1 1]]\n"
     ]
    }
   ],
   "source": [
    "vᵀ = array([[1]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input is vector, output is scalar, non-default gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep everything else the same as above but pass a non-default `gradient` with the value 100 to `backward()` instead of the default value 1. Still, $x=[x_1,x_2]=[1,2]$ is a vector and $y=\\sum x_i$ is a scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "x.grad: tensor([100., 100.])\n"
     ]
    }
   ],
   "source": [
    "x = tensor([1., 2.], requires_grad=True)\n",
    "print('x:', x)\n",
    "y = sum(x)\n",
    "gradient_value = 100.\n",
    "y.backward(tensor(gradient_value)) \n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as setting the value `100` for $v^T$, and we can see $v^T \\cdot J$ still matches `x.grad`. Still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J:\n",
      "[[1 1]]\n",
      "vᵀ: [[100.]]\n",
      "vᵀ・J: [[100. 100.]]\n"
     ]
    }
   ],
   "source": [
    "J = array([[1, 1]])\n",
    "print('J:')\n",
    "print(J)\n",
    "\n",
    "\n",
    "vᵀ = array([[gradient_value,]])\n",
    "print('vᵀ:', vᵀ)\n",
    "print('vᵀ・J:', vᵀ@J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastaiv1-cpu",
   "language": "python",
   "name": "fastaiv1-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
